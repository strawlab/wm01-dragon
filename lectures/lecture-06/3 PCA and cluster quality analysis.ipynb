{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and cluster quality analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://matplotlib.org/stable/users/explain/backends.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A 'single cell seq' type dataset\n",
    "\n",
    "We are going to skip a bunch of biology and wet lab realities and generate a toy dataset with many characteristics of a real single cell sequencing experiment. In these types of experiments, RNA is extracted from many individual cells and the number of copies (also known as number of \"reads\") of RNA molecules for particular genes is quantified. Here we generate a dataset in which we quantify the number of reads for 200 genes from many cells which belong to some number of cell types. Typically in such a 'single cell seq' dataset, the exact number of cell types are not known in advance nor is the the correspondence between cells and cell types. Typically we need to figure this out through data analysis. We are going to use PCA and clustering to perform these jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some toy data with known characteristics.\n",
    "#\n",
    "# NOTE: Don't read this code upon first run-through of the notebook. This\n",
    "# would spoil the rest of the work we do below to \"discover\" what was done\n",
    "# here in the data generation step.\n",
    "\n",
    "n_samples_per_cluster = 100\n",
    "n_dim = 200\n",
    "n_clusters = 4\n",
    "\n",
    "def gen_covar_mat(ndim):\n",
    "    result = np.zeros((ndim,ndim))\n",
    "    for i in range(ndim):\n",
    "        for j in range(i,ndim):\n",
    "            value = np.random.randn(1,1)[0]\n",
    "            result[i,j] = value\n",
    "            result[j,i] = value\n",
    "    return result\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "X_train = []\n",
    "for i in range(n_clusters):\n",
    "\n",
    "    center = 10*np.random.randn(n_dim) + np.array([10]*n_dim)\n",
    "    \n",
    "    C = gen_covar_mat(n_dim)\n",
    "    stretched_gaussian = np.dot(np.random.randn(n_samples_per_cluster, n_dim), C) + center\n",
    "\n",
    "    big_positive = (100*stretched_gaussian + np.array([1000]*n_dim))\n",
    "    \n",
    "    big_positive = np.clip(big_positive,0, np.inf)\n",
    "    big_positive = np.round(big_positive)    \n",
    "    X_train.append( big_positive )\n",
    "\n",
    "\n",
    "# concatenate the datasets into the final training set\n",
    "X_train = np.vstack(X_train)\n",
    "\n",
    "# shuffle the rows\n",
    "idxs = np.arange(len(X_train))\n",
    "idxs = np.random.shuffle(idxs)\n",
    "X_train = X_train[idxs][0]\n",
    "\n",
    "x = np.linspace(-20., 30.)\n",
    "y = np.linspace(-20., 40.)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(X_train[:, 0], X_train[:, 1], .8)\n",
    "# plt.xlabel(\"num reads (gene 0)\")\n",
    "# plt.ylabel(\"num reads (gene 1)\");\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], '.')\n",
    "ax.set_xlabel('gene 0')\n",
    "ax.set_ylabel('gene 1')\n",
    "ax.set_zlabel('gene 2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {}\n",
    "for i in range(n_dim):\n",
    "    name = 'gene %d'%i\n",
    "    data[name] = X_train[:,i].astype(np.int64)\n",
    "\n",
    "df = pd.DataFrame(data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a first look at this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first impressions of the data\n",
    "\n",
    "So, how does our data look? At first glance it looks... like a bunch of random numbers with no real structure! But could there be some structure? For example, above we learned that although many cells have been sequenced, we expect these are from only a very limited number of cell types.\n",
    "\n",
    "How can we figure out something about these cell types?\n",
    "\n",
    "Let's make use of principal component analysis (PCA) and clustering from scikit-learn as some of the first tools in our toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to plain numpy\n",
    "\n",
    "While Pandas is very convenient for many things, scikit learn uses plain numpy arrays and generally works best when the datatype is a floating point number rather than an integer. Let's do this conversion now and call our data `X`.\n",
    "\n",
    "We need to make a copy of this so that it is \"C contiguous\" (this is the default numpy layout of the data but when creating a view of other data, the internal layout can be different)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy(dtype=np.float64).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "Let's first run PCA on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of our analysis are stored in the variable `pca`. We can use this to project our original high dimensional data into its principle components and plot just the first dimensions in this \"principle component space\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(projected[:,0], projected[:,1], '.')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(projected[:,0], projected[:,1], projected[:,2], '.')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure in the PCA space\n",
    "\n",
    "Now, what do you notice about the data in this PCA space?\n",
    "\n",
    "Now, instead of looking like a structure-free blob, we seem to have some structure. What kind of structure do we have?\n",
    "\n",
    "\"N clusters\", I hope you are thinking. Biologically speaking, we are now guessing that there were this many cell types in our original sample.\n",
    "\n",
    "## PCA explained variance\n",
    "\n",
    "One of the first questions about PCA is how much of the variance in our data are \"explained\" by the first N components of the projected data. Let's plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),'.-')\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mini batch K-Means\n",
    "\n",
    "Given that it looks like our data may have some clusters, let's try find these clusters using mini batch K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbk = MiniBatchKMeans(n_clusters=n_clusters, batch_size=6, random_state=42, n_init='auto').fit(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting the clustering results in the original \"number of reads\" space\n",
    "\n",
    "Let's first plot the our raw read data in a scatter plot like above, but colored according to our cluster label. We will also plot our cluster centers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbk_means_cluster_centers = mbk.cluster_centers_\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "x_gene_idx = 0\n",
    "y_gene_idx = 1\n",
    "\n",
    "for k in range(n_clusters):\n",
    "    my_members = mbk_means_labels == k\n",
    "    cluster_center = mbk_means_cluster_centers[k]\n",
    "    line, = ax.plot(X[my_members, x_gene_idx], X[my_members, 1], '.', markersize=5)\n",
    "    ax.plot(cluster_center[x_gene_idx], cluster_center[1], 'o', markersize=10, markeredgecolor='black', markerfacecolor=line.get_color())\n",
    "ax.set_xlabel('gene %d' % x_gene_idx)\n",
    "ax.set_ylabel('gene %d' % y_gene_idx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting the clustering results in PCA space\n",
    "\n",
    "Hmm, the plot above was not too informative. It does not seem to show obvious clusters in the data, and the points look very interwoven with others, at least for these two genes.\n",
    "\n",
    "Let's re-plot our cluster assignments, but this time using the projection into PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "projected_centers = pca.transform(mbk.cluster_centers_)\n",
    "\n",
    "for k in range(n_clusters):\n",
    "    my_members = mbk_means_labels == k\n",
    "    projected_cluster_center = projected_centers[k]\n",
    "    line, = ax.plot(projected[my_members, 0], projected[my_members, 1], '.',\n",
    "                    markersize=1.8)\n",
    "    ax.plot(projected_cluster_center[0], projected_cluster_center[1], 'o',\n",
    "            markersize=10, markeredgecolor='black', markerfacecolor=line.get_color())\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## silhouette analysis with K-means\n",
    "\n",
    "OK, so that last plot (in PCA space) is looking better. The automatically detected clusters seem to agree with the idea we had from just looking at the data. Let's now use a silhouette analysis as one way to check whether this was a particularly good number of clusters for this data.\n",
    "\n",
    "\n",
    "## Selecting the number of clusters with silhouette analysis on KMeans clustering\n",
    "\n",
    "(This section modified from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html )\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the\n",
    "resulting clusters. The silhouette plot displays a measure of how close each\n",
    "point in one cluster is to points in the neighboring clusters and thus provides\n",
    "a way to assess parameters like number of clusters visually. This measure has a\n",
    "range of [-1, 1].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate\n",
    "that the sample is far away from the neighboring clusters. A value of 0\n",
    "indicates that the sample is on or very close to the decision boundary between\n",
    "two neighboring clusters and negative values indicate that those samples might\n",
    "have been assigned to the wrong cluster.\n",
    "\n",
    "In this example, the silhouette analysis is used to inform us about an optimal value for\n",
    "the best number of clusters.\n",
    "\n",
    "Also from the thickness of the silhouette plot the cluster size can be\n",
    "visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from scipy.spatial import distance\n",
    "import sklearn.datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# from https://stats.stackexchange.com/a/251169/24333\n",
    "def compute_bic(kmeans,X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for a given clusters\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    "\n",
    "    X     :  multidimension np array of data points\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    BIC value\n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = [kmeans.cluster_centers_]\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    "\n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], \n",
    "             'euclidean')**2) for i in range(m)])\n",
    "\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    "\n",
    "    # Note: our implementation of BIC has an inverted sign compared to that in the\n",
    "    # e.g. wikipedia article, and thus the best fit has the highest value.\n",
    "    \n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    "\n",
    "    return(BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the plots below, we need to turn off the interactive Jupyter matplotlib notebook mode.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://scikit-learn.org/0.21/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "bic_score = []\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10, n_init='auto')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    \n",
    "    this_bic = compute_bic(clusterer,X)\n",
    "    bic_score.append(this_bic)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis, n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIC - Bayesian information criterion\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bayesian_information_criterion\n",
    "\n",
    "Note: our implementation of BIC has an inverted sign compared to that in the wikipedia article, and thus the best fit has the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.plot(range_n_clusters,bic_score,'o-')\n",
    "ax.set_xlabel(\"num clusters (k)\")\n",
    "ax.set_ylabel(\"BIC\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
